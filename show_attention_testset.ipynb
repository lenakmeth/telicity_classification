{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, \\\n",
    "AlbertForSequenceClassification, XLNetForSequenceClassification, CamembertForSequenceClassification, \\\n",
    "FlaubertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "BertTokenizer, RobertaTokenizer, AlbertTokenizer, XLNetTokenizer, CamembertTokenizer, FlaubertTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'bert-base-cased'\n",
    "verb_segment_ids = 'yes'\n",
    "num_epochs = 4\n",
    "use_segment_ids = True\n",
    "# logger = open(transformer_model + '_' + str(use_segment_ids) + '.log', 'w')\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_real_verb_idx(sentence_list, encoded_sentence, tokenizer):\n",
    "    \n",
    "    encoded_verb = tokenizer.encode(sentence_list[1])\n",
    "\n",
    "    try:\n",
    "        if len(encoded_verb) == 3: # verb as is + [CLS] + [SEP]\n",
    "            verb_idx = [encoded_sentence.index(encoded_verb[1])]\n",
    "        else:\n",
    "            decoded_verb = tokenizer.convert_ids_to_tokens(encoded_verb)\n",
    "            decoded_sent = tokenizer.convert_ids_to_tokens(encoded_sentence)\n",
    "#             print(decoded_sent)\n",
    "#             print(decoded_verb)\n",
    "            verb_segment = [seg for seg in decoded_verb \n",
    "                            if not any(seg.startswith(x) for x in ['[', '<'])]\n",
    "            verb_idx = [decoded_sent.index(seg) for seg in verb_segment]\n",
    "            \n",
    "        return verb_idx\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize_and_pad(transformer_model, sentences):\n",
    "    \"\"\" We are using .encode_plus. This does not make specialized attn masks \n",
    "        like in our selectional preferences experiment. Revert to .encode if\n",
    "        necessary.\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    segment_ids = [] # token type ids\n",
    "    attention_masks = []\n",
    "    \n",
    "    if transformer_model.split(\"-\")[0] == 'bert':\n",
    "        tok = BertTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'roberta':\n",
    "        tok = RobertaTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'albert':\n",
    "        tok = AlbertTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'xlnet':\n",
    "        tok = XLNetTokenizer.from_pretrained(transformer_model)\n",
    "    elif 'camembert' in transformer_model:\n",
    "        tok = CamembertTokenizer.from_pretrained(transformer_model)\n",
    "    elif 'flaubert' in transformer_model:\n",
    "        tok = FlaubertTokenizer.from_pretrained(transformer_model)\n",
    "\n",
    "    for sent in sentences:\n",
    "        sentence = sent[0]\n",
    "\n",
    "        # encode_plus is a prebuilt function that will make input_ids, \n",
    "        # add padding/truncate, add special tokens, + make attention masks \n",
    "        encoded_dict = tok.encode_plus(\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,      # Pad & truncate all sentences.\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True, # Construct attn. masks.\n",
    "                        # return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        # Add segment ids, add 1 for verb idx\n",
    "        segment_id = [0] * 128        \n",
    "        verb_idx = find_real_verb_idx(sent, encoded_dict['input_ids'], tok)       \n",
    "        if verb_idx: # if False, the verb is not in the first 128 tokens\n",
    "            for idx in verb_idx:\n",
    "                segment_id[idx] = 1            \n",
    "        segment_ids.append(segment_id)    \n",
    "\n",
    "    return input_ids, attention_masks, segment_ids\n",
    "\n",
    "\n",
    "def decode_result(transformer_model, encoded_sequence):\n",
    "\n",
    "    if transformer_model.split(\"-\")[0] == 'bert':\n",
    "        tok = BertTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'roberta':\n",
    "        tok = RobertaTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'albert':\n",
    "        tok = AlbertTokenizer.from_pretrained(transformer_model)\n",
    "    elif transformer_model.split(\"-\")[0] == 'xlnet':\n",
    "        tok = XLNetTokenizer.from_pretrained(transformer_model)\n",
    "    elif 'camembert' in transformer_model:\n",
    "        tok = CamembertTokenizer.from_pretrained(transformer_model)\n",
    "    elif 'flaubert' in transformer_model:\n",
    "        tok = FlaubertTokenizer.from_pretrained(transformer_model)\n",
    "    \n",
    "    # decode + remove special tokens\n",
    "    tokens_to_remove = ['[PAD]', '<pad>', '<s>', '</s>']\n",
    "    decoded_sequence = [w.replace('Ġ', '').replace('▁', '').replace('</w>', '')\n",
    "                        for w in list(tok.convert_ids_to_tokens(encoded_sequence))\n",
    "                        if not w.strip() in tokens_to_remove]\n",
    "    \n",
    "    return decoded_sequence\n",
    "\n",
    "def read_sents(path, marker):\n",
    "    \"\"\" Read the .tsv files with the annotated sentences. \n",
    "        File format: sent_id, sentence, verb, verb_idx, label\"\"\"\n",
    "\n",
    "    def open_file(file):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                l = line.strip().split('\\t')\n",
    "                sentences.append([l[-4], l[-3], int(l[-2])])\n",
    "                labels.append(int(l[-1]))\n",
    "                \n",
    "            return sentences,labels\n",
    "        \n",
    "    train_sentences, train_labels = open_file(path + '/' + marker + '_train.tsv')    \n",
    "    val_sentences, val_labels = open_file(path +  '/' + marker + '_val.tsv')\n",
    "    test_sentences, test_labels = open_file(path +  '/' + marker + '_test.tsv')\n",
    "\n",
    "    return train_sentences, train_labels, val_sentences, val_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 3\n",
    "example_sep = 3\n",
    "word_height = 1\n",
    "pad = 0.1\n",
    "\n",
    "def plot_attn(sentence, attentions, layer, heads):\n",
    "    \"\"\"Plotspredicted_labelsntion maps for the given example and attention heads.\"\"\"\n",
    "\n",
    "    for ei, head in enumerate(heads):\n",
    "        yoffset = 1\n",
    "        xoffset = ei * width * example_sep\n",
    "\n",
    "        attn = attentions[layer][head]\n",
    "        attn = np.array(attn)\n",
    "        attn /= attn.sum(axis=-1, keepdims=True)\n",
    "        words = sentence\n",
    "        n_words = len(words)\n",
    "\n",
    "        for position, word in enumerate(words):\n",
    "            plt.text(xoffset + 0, yoffset - position * word_height, word,\n",
    "                   ha=\"right\", va=\"center\")\n",
    "            plt.text(xoffset + width, yoffset - position * word_height, word,\n",
    "                   ha=\"left\", va=\"center\")\n",
    "        for i in range(1, n_words):\n",
    "            for j in range(1, n_words):\n",
    "                plt.plot([xoffset + pad, xoffset + width - pad],\n",
    "                 [yoffset - word_height * i, yoffset - word_height * j],\n",
    "                 color=\"blue\", linewidth=1, alpha=attn[i, j].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uses verb segment ids: ' + str(use_segment_ids))\n",
    "print('Model: ' + transformer_model)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# PARAMETERS\n",
    "transformer_model = transformer_model\n",
    "epochs = num_epochs\n",
    "\n",
    "# read friedrich sentences, choose labels of telicity/duration\n",
    "train_sentences, train_labels, val_sentences, val_labels, \\\n",
    "test_sentences, test_labels = read_sents(\"data/friedrich_captions_data\", 'telicity')\n",
    "\n",
    "# make input ids, attention masks, segment ids, depending on the model we will use\n",
    "train_inputs, train_masks, train_segments = tokenize_and_pad(transformer_model, train_sentences)\n",
    "print('Loaded sentences and converted.')\n",
    "# logger.write('\\nTrain set: ' + str(len(train_inputs)))\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "if use_segment_ids:\n",
    "    train_segments = torch.tensor(train_segments)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "if use_segment_ids:\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels, train_segments)\n",
    "else:\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "if ( transformer_model).split(\"-\")[0] == 'bert':\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        transformer_model,\n",
    "        num_labels = 2, \n",
    "        output_attentions = True,\n",
    "        output_hidden_states = False, \n",
    "    )\n",
    "elif ( transformer_model).split(\"-\")[0] == 'roberta':\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        transformer_model, \n",
    "        num_labels = 2,   \n",
    "        output_attentions = True,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "elif ( transformer_model).split(\"-\")[0] == 'albert':\n",
    "    model = AlbertForSequenceClassification.from_pretrained(\n",
    "        transformer_model, \n",
    "        num_labels = 2,   \n",
    "        output_attentions = True,\n",
    "        output_hidden_states = False, \n",
    "    )\n",
    "elif ( transformer_model).split(\"-\")[0] == 'xlnet':\n",
    "    model = XLNetForSequenceClassification.from_pretrained(\n",
    "        transformer_model, \n",
    "        num_labels = 2,   \n",
    "        output_attentions = True,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "elif 'flaubert' in  transformer_model:\n",
    "    model = FlaubertForSequenceClassification.from_pretrained(\n",
    "        transformer_model, \n",
    "        num_labels = 2,   \n",
    "        output_attentions = True,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "elif 'camembert' in  transformer_model:\n",
    "    model = CamembertForSequenceClassification.from_pretrained(\n",
    "        transformer_model, \n",
    "        num_labels = 2,   \n",
    "        output_attentions = True, \n",
    "        output_hidden_states = False, \n",
    "    )\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "#     logger.write('\\n\\t======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#     logger.write('\\nTraining...')\n",
    "    print('\\n\\t======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('\\nTraining...')\n",
    "#     t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "           \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        if use_segment_ids:\n",
    "            b_segments = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        if use_segment_ids:\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=b_segments, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "#     logger.write('\\n\\tAverage training loss: {0:.2f}'.format(avg_train_loss))\n",
    "#     logger.write('\\n\\tTraining epoch took: {:}'.format(format_time(time.time() - t0)))\n",
    "    print('\\n\\tAverage training loss: {0:.2f}'.format(avg_train_loss))\n",
    "#     print('\\n\\tTraining epoch took: {:}'.format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open unseen test set\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "    \n",
    "with open('data/unseen_tests/telicity_unseen.tsv', 'r', encoding='utf-8') as f: # qualitative test 1\n",
    "# with open('data/unseen_tests/telicity_min_pairs.tsv', 'r', encoding='utf-8') as f:     # minimal pairs test set\n",
    "# with open('data/unseen_tests/telicity_more_tests.tsv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split('\\t')\n",
    "        test_sentences.append([l[-4], l[-3], int(l[-2])])\n",
    "        test_labels.append(int(l[-1]))\n",
    "\n",
    "# use_segment_ids = False\n",
    "test_inputs, test_masks, test_segments = tokenize_and_pad(transformer_model, test_sentences)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_segments = torch.tensor(test_segments)\n",
    "\n",
    "# Return attentions for each sentence of the test set, attentions per sentence (not batched per layer)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_inputs = []\n",
    "sent_attentions = []\n",
    "sentences = []\n",
    "predicted_labels = []\n",
    "prob_prediction = []\n",
    "\n",
    "for inputs in test_inputs:\n",
    "    \n",
    "    test_input = inputs.resize(1, 128)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(test_input, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=None)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    attentions = outputs[1]\n",
    "    \n",
    "    log_probs = F.softmax(Variable(torch.from_numpy(logits)), dim=-1)\n",
    "\n",
    "    predicted_labels += np.argmax(logits, axis=1).flatten().tolist()\n",
    "    prob_prediction += log_probs.tolist()\n",
    "    \n",
    "    sentence = decode_result(transformer_model, inputs)\n",
    "    sentences.append(sentence)\n",
    "    len_sequence = len(sentence)\n",
    "    \n",
    "    temp_attentions = [] # turn attention to (layer, head, size, size)\n",
    "    \n",
    "    for layer in attentions:\n",
    "        temp = torch.squeeze(layer) #remove dimension of batch size = 1\n",
    "        temp = np.array(temp)[:, :len_sequence, :len_sequence]\n",
    "        temp_attentions.append(temp)\n",
    "        \n",
    "    sent_attentions.append(np.asarray(temp_attentions))\n",
    "\n",
    "logz = open('plots/unseen/sent_indices.txt', 'w')\n",
    "logz.write('ID\\tSentence\\tLabel\\tPred.\\tProbabilities\\n')\n",
    "\n",
    "for sent_idx in range(len(sentences)):\n",
    "    print(sent_idx)\n",
    "    save_path = 'plots/unseen/' + str(sent_idx) + '/' + transformer_model + '_' + verb_segment_ids +   '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)    \n",
    "    logz.write('\\t'.join([str(sent_idx), ' '.join(sentences[sent_idx]), \n",
    "                            str(test_labels[sent_idx]), \n",
    "                            str(predicted_labels[sent_idx]),\n",
    "                            str(prob_prediction[sent_idx][0]),\n",
    "                            str(prob_prediction[sent_idx][1])\n",
    "                         ]))\n",
    "    logz.write('\\n')\n",
    "    for layer in range(12):\n",
    "        heads = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#         print('\\nLayer:', layer+1, '\\n')\n",
    "        plt.figure(figsize=(50, 4))\n",
    "        plt.axis(\"off\")\n",
    "        plot_attn(sentences[sent_idx], sent_attentions[sent_idx], layer, heads)\n",
    "        plt.savefig(save_path + str(layer+1) + '.png')\n",
    "        \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    # sent_idx = 2 # target_sentence\n",
    "    len_sequence = len(sentences[sent_idx])\n",
    "\n",
    "    attentions_pos = sent_attentions[sent_idx]\n",
    "    attentions_pos = torch.FloatTensor(attentions_pos).permute(2,1,0,3)\n",
    "\n",
    "    verb_idx = test_segments[sent_idx].tolist().index(1) + 1 # add 1 for the [CLS] token\n",
    "    \n",
    "    #Plot Attention for specifically the verb\n",
    "\n",
    "    cols = 2\n",
    "    heads = 12\n",
    "    rows = int(heads/cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize = (14,30))\n",
    "    axes = axes.flat\n",
    "#     print('Sentence: ', ' '.join(sentences[sent_idx]))\n",
    "#     print ('Attention weights for token: ', sentences[sent_idx][verb_idx])\n",
    "\n",
    "    for i, att in enumerate(attentions_pos[verb_idx]):\n",
    "        #im = axes[i].imshow(att, cmap='gray')\n",
    "        sns.heatmap(att, vmin = 0, vmax = 1, ax = axes[i], xticklabels = sentences[sent_idx])\n",
    "        axes[i].set_title(f'head - {i+1} ' )\n",
    "        axes[i].set_ylabel('layers')\n",
    "        plt.savefig(save_path + 'verb_attn.png')\n",
    "    plt.cla()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open min test set\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "    \n",
    "# with open('data/unseen_tests/telicity_unseen.tsv', 'r', encoding='utf-8') as f: # qualitative test 1\n",
    "with open('data/unseen_tests/telicity_min_pairs.tsv', 'r', encoding='utf-8') as f:     # minimal pairs test set\n",
    "# with open('data/unseen_tests/telicity_more_tests.tsv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split('\\t')\n",
    "        test_sentences.append([l[-4], l[-3], int(l[-2])])\n",
    "        test_labels.append(int(l[-1]))\n",
    "\n",
    "# use_segment_ids = False\n",
    "test_inputs, test_masks, test_segments = tokenize_and_pad(transformer_model, test_sentences)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_segments = torch.tensor(test_segments)\n",
    "\n",
    "# Return attentions for each sentence of the test set, attentions per sentence (not batched per layer)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_inputs = []\n",
    "sent_attentions = []\n",
    "sentences = []\n",
    "predicted_labels = []\n",
    "prob_prediction = []\n",
    "\n",
    "for inputs in test_inputs:\n",
    "    \n",
    "    test_input = inputs.resize(1, 128)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(test_input, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=None)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    attentions = outputs[1]\n",
    "    \n",
    "    log_probs = F.softmax(Variable(torch.from_numpy(logits)), dim=-1)\n",
    "\n",
    "    predicted_labels += np.argmax(logits, axis=1).flatten().tolist()\n",
    "    prob_prediction += log_probs.tolist()\n",
    "    \n",
    "    sentence = decode_result(transformer_model, inputs)\n",
    "    sentences.append(sentence)\n",
    "    len_sequence = len(sentence)\n",
    "    \n",
    "    temp_attentions = [] # turn attention to (layer, head, size, size)\n",
    "    \n",
    "    for layer in attentions:\n",
    "        temp = torch.squeeze(layer) #remove dimension of batch size = 1\n",
    "        temp = np.array(temp)[:, :len_sequence, :len_sequence]\n",
    "        temp_attentions.append(temp)\n",
    "        \n",
    "    sent_attentions.append(np.asarray(temp_attentions))\n",
    "    \n",
    "logz = open('plots/min_pairs/sent_indices.txt', 'w')\n",
    "logz.write('ID\\tSentence\\tLabel\\tPred.\\tProbabilities\\n')\n",
    "\n",
    "for sent_idx in range(len(sentences)):\n",
    "    print(sent_idx)\n",
    "    save_path = 'plots/min_pairs/' + str(sent_idx) + '/' + transformer_model + '_' + verb_segment_ids +   '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)    \n",
    "    logz.write('\\t'.join([str(sent_idx), ' '.join(sentences[sent_idx]), \n",
    "                            str(test_labels[sent_idx]), \n",
    "                            str(predicted_labels[sent_idx]),\n",
    "                            str(prob_prediction[sent_idx][0]),\n",
    "                            str(prob_prediction[sent_idx][1])\n",
    "                         ]))\n",
    "    logz.write('\\n')\n",
    "    for layer in range(12):\n",
    "        heads = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#         print('\\nLayer:', layer+1, '\\n')\n",
    "        plt.figure(figsize=(50, 4))\n",
    "        plt.axis(\"off\")\n",
    "        plot_attn(sentences[sent_idx], sent_attentions[sent_idx], layer, heads)\n",
    "        plt.savefig(save_path + str(layer+1) + '.png')\n",
    "        \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    # sent_idx = 2 # target_sentence\n",
    "    len_sequence = len(sentences[sent_idx])\n",
    "\n",
    "    attentions_pos = sent_attentions[sent_idx]\n",
    "    attentions_pos = torch.FloatTensor(attentions_pos).permute(2,1,0,3)\n",
    "\n",
    "    verb_idx = test_segments[sent_idx].tolist().index(1) + 1 # add 1 for the [CLS] token\n",
    "    \n",
    "    #Plot Attention for specifically the verb\n",
    "\n",
    "    cols = 2\n",
    "    heads = 12\n",
    "    rows = int(heads/cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize = (14,30))\n",
    "    axes = axes.flat\n",
    "#     print('Sentence: ', ' '.join(sentences[sent_idx]))\n",
    "#     print ('Attention weights for token: ', sentences[sent_idx][verb_idx])\n",
    "\n",
    "    for i, att in enumerate(attentions_pos[verb_idx]):\n",
    "        #im = axes[i].imshow(att, cmap='gray')\n",
    "        sns.heatmap(att, vmin = 0, vmax = 1, ax = axes[i], xticklabels = sentences[sent_idx])\n",
    "        axes[i].set_title(f'head - {i+1} ' )\n",
    "        axes[i].set_ylabel('layers')\n",
    "        plt.savefig(save_path + 'verb_attn.png')\n",
    "    plt.cla()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open more test set\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "    \n",
    "# with open('data/unseen_tests/telicity_unseen.tsv', 'r', encoding='utf-8') as f: # qualitative test 1\n",
    "# with open('data/unseen_tests/telicity_min_pairs.tsv', 'r', encoding='utf-8') as f:     # minimal pairs test set\n",
    "with open('data/unseen_tests/telicity_more_tests.tsv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split('\\t')\n",
    "        test_sentences.append([l[-4], l[-3], int(l[-2])])\n",
    "        test_labels.append(int(l[-1]))\n",
    "\n",
    "# use_segment_ids = False\n",
    "test_inputs, test_masks, test_segments = tokenize_and_pad(transformer_model, test_sentences)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_segments = torch.tensor(test_segments)\n",
    "\n",
    "# Return attentions for each sentence of the test set, attentions per sentence (not batched per layer)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_inputs = []\n",
    "sent_attentions = []\n",
    "sentences = []\n",
    "predicted_labels = []\n",
    "prob_prediction = []\n",
    "\n",
    "for inputs in test_inputs:\n",
    "    \n",
    "    test_input = inputs.resize(1, 128)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(test_input, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=None)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    attentions = outputs[1]\n",
    "    \n",
    "    log_probs = F.softmax(Variable(torch.from_numpy(logits)), dim=-1)\n",
    "\n",
    "    predicted_labels += np.argmax(logits, axis=1).flatten().tolist()\n",
    "    prob_prediction += log_probs.tolist()\n",
    "    \n",
    "    sentence = decode_result(transformer_model, inputs)\n",
    "    sentences.append(sentence)\n",
    "    len_sequence = len(sentence)\n",
    "    \n",
    "    temp_attentions = [] # turn attention to (layer, head, size, size)\n",
    "    \n",
    "    for layer in attentions:\n",
    "        temp = torch.squeeze(layer) #remove dimension of batch size = 1\n",
    "        temp = np.array(temp)[:, :len_sequence, :len_sequence]\n",
    "        temp_attentions.append(temp)\n",
    "        \n",
    "    sent_attentions.append(np.asarray(temp_attentions))\n",
    "    \n",
    "logz = open('plots/min_pairs/sent_indices.txt', 'w')\n",
    "logz.write('ID\\tSentence\\tLabel\\tPred.\\tProbabilities\\n')\n",
    "\n",
    "for sent_idx in range(len(sentences)):\n",
    "    print(sent_idx)\n",
    "    save_path = 'plots/more_tests/' + str(sent_idx) + '/' + transformer_model + '_' + verb_segment_ids +   '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)    \n",
    "    logz.write('\\t'.join([str(sent_idx), ' '.join(sentences[sent_idx]), \n",
    "                            str(test_labels[sent_idx]), \n",
    "                            str(predicted_labels[sent_idx]),\n",
    "                            str(prob_prediction[sent_idx][0]),\n",
    "                            str(prob_prediction[sent_idx][1])\n",
    "                         ]))\n",
    "    logz.write('\\n')\n",
    "    for layer in range(12):\n",
    "        heads = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#         print('\\nLayer:', layer+1, '\\n')\n",
    "        plt.figure(figsize=(50, 4))\n",
    "        plt.axis(\"off\")\n",
    "        plot_attn(sentences[sent_idx], sent_attentions[sent_idx], layer, heads)\n",
    "        plt.savefig(save_path + str(layer+1) + '.png')\n",
    "        \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    # sent_idx = 2 # target_sentence\n",
    "    len_sequence = len(sentences[sent_idx])\n",
    "\n",
    "    attentions_pos = sent_attentions[sent_idx]\n",
    "    attentions_pos = torch.FloatTensor(attentions_pos).permute(2,1,0,3)\n",
    "\n",
    "    verb_idx = test_segments[sent_idx].tolist().index(1) + 1 # add 1 for the [CLS] token\n",
    "    \n",
    "    #Plot Attention for specifically the verb\n",
    "\n",
    "    cols = 2\n",
    "    heads = 12\n",
    "    rows = int(heads/cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize = (14,30))\n",
    "    axes = axes.flat\n",
    "#     print('Sentence: ', ' '.join(sentences[sent_idx]))\n",
    "#     print ('Attention weights for token: ', sentences[sent_idx][verb_idx])\n",
    "\n",
    "    for i, att in enumerate(attentions_pos[verb_idx]):\n",
    "        #im = axes[i].imshow(att, cmap='gray')\n",
    "        sns.heatmap(att, vmin = 0, vmax = 1, ax = axes[i], xticklabels = sentences[sent_idx])\n",
    "        axes[i].set_title(f'head - {i+1} ' )\n",
    "        axes[i].set_ylabel('layers')\n",
    "        plt.savefig(save_path + 'verb_attn.png')\n",
    "    plt.cla()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
