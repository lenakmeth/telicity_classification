{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sents(path, marker):\n",
    "    \"\"\" Read the .tsv files with the annotated sentences. \n",
    "        File format: sent_id, sentence, verb, verb_idx, label\"\"\"\n",
    "\n",
    "    def open_file(file):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                l = line.strip().split('\\t')\n",
    "                sentences.append(l[-4])\n",
    "                labels.append(int(l[-1]))\n",
    "        \n",
    "            return sentences,labels\n",
    "        \n",
    "    train_sentences, train_labels = open_file(path + '/' + marker + '_train.tsv')    \n",
    "    val_sentences, val_labels = open_file(path +  '/' + marker + '_val.tsv')\n",
    "    test_sentences, test_labels = open_file(path +  '/' + marker + '_test.tsv')\n",
    "\n",
    "    return train_sentences, train_labels, val_sentences, val_labels, test_sentences, test_labels\n",
    "\n",
    "train_sentences, train_labels, val_sentences, val_labels, \\\n",
    "    test_sentences, test_labels = read_sents('acl_data', 'duration')\n",
    "\n",
    "X = train_sentences + val_sentences + test_sentences\n",
    "y = train_labels + val_labels + test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split train & test\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=RANDOM_STATE)\n",
    "\n",
    "# Tokenize and transform to integer index\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "maxlen = max(len(x) for x in X_train) # longest text in train set\n",
    "\n",
    "# Add pading to ensure all vectors have same dimensionality\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN architecture\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "\n",
    "predictions = model.predict_classes(X_test, verbose=0)\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_sample = ['this is a sample text']\n",
    "X_sample = tokenizer.texts_to_sequences(X_sample)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=maxlen)\n",
    "\n",
    "y_sample = model.predict_classes(X_sample).flatten().tolist()\n",
    "\n",
    "print('Prediction: ',sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')\n",
    "model_load = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
